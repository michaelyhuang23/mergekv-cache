=== RULER Benchmark Evaluation ===
Model: deepseek-ai/DeepSeek-R1-Distill-Llama-8B
Compression ratio: 8x
Max sequence length: 8192
Output directory: results
Timestamp: 20250326_145430

No specific compression method specified, running both Q-Filters and K-norm...
Loading model deepseek-ai/DeepSeek-R1-Distill-Llama-8B...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.94s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.74s/it]
Using Q-Filters compression with ratio 8x
Running RULER benchmark with q_filters compression...
Traceback (most recent call last):
  File "/home/dhruv/kvcash/scripts/run_ruler_benchmark.py", line 146, in <module>
    main() 
  File "/home/dhruv/kvcash/scripts/run_ruler_benchmark.py", line 126, in main
    results["q_filters"] = run_evaluation(args, model, tokenizer, "q_filters")
  File "/home/dhruv/kvcash/scripts/run_ruler_benchmark.py", line 93, in run_evaluation
    results = evaluator.simple_evaluate(
  File "/home/dhruv/ttft/eval/lm-evaluation-harness/lm_eval/utils.py", line 397, in _wrapper
    return fn(*args, **kwargs)
TypeError: simple_evaluate() got an unexpected keyword argument 'metadata'
Loading model deepseek-ai/DeepSeek-R1-Distill-Llama-8B...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.45s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.56s/it]
Using K-norm compression with ratio 8x
Running RULER benchmark with k_norm compression...
Traceback (most recent call last):
  File "/home/dhruv/kvcash/scripts/run_ruler_benchmark.py", line 146, in <module>
    main() 
  File "/home/dhruv/kvcash/scripts/run_ruler_benchmark.py", line 133, in main
    results["k_norm"] = run_evaluation(args, model, tokenizer, "k_norm")
  File "/home/dhruv/kvcash/scripts/run_ruler_benchmark.py", line 93, in run_evaluation
    results = evaluator.simple_evaluate(
  File "/home/dhruv/ttft/eval/lm-evaluation-harness/lm_eval/utils.py", line 397, in _wrapper
    return fn(*args, **kwargs)
TypeError: simple_evaluate() got an unexpected keyword argument 'metadata'
Evaluation completed. Results saved to results
